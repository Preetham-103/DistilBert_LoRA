{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall pyarrow -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gqbRLhPfqX0j",
        "outputId": "67e23cc9-45dd-4adf-c1a3-9c7d20179073"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pyarrow 14.0.1\n",
            "Uninstalling pyarrow-14.0.1:\n",
            "  Successfully uninstalled pyarrow-14.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyarrow==14.0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "a9asGf7bqeiZ",
        "outputId": "4a6bb6dc-c5d7-43a5-ba3d-25d9c2591e22"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyarrow==14.0.1\n",
            "  Using cached pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow==14.0.1) (1.26.4)\n",
            "Using cached pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n",
            "Installing collected packages: pyarrow\n",
            "Successfully installed pyarrow-14.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7O4QBGGxpNBz",
        "outputId": "7d061a78-a6da-490d-81b9-968df31e977f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "I4ic2BJqw8Nl",
        "outputId": "609e1d6e-3fec-4ea8-9411-05b93aa5543b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.4.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from peft import get_peft_model, LoraConfig"
      ],
      "metadata": {
        "id": "EL48DCpnuS61"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('IMDB Dataset.csv', on_bad_lines='skip', delimiter=',', quoting=3)\n",
        "df = df.sample(frac=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "u59xS1OaHvZg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'] = df['sentiment'].map({'positive': 1, 'negative': 0})  # Map sentiment to labels\n",
        "df.dropna(inplace=True)  # Remove missing values"
      ],
      "metadata": {
        "id": "LBgoRKz5dkqK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(df['review'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "fIBJNDK9iW8P"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name,num_labels=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmrFav_xI3_n",
        "outputId": "e43c5ec1-2773-4e7f-b93f-03598e3a421b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_modules = [\n",
        "    \"distilbert.transformer.layer.0.attention.q_lin\",\n",
        "    \"distilbert.transformer.layer.0.attention.k_lin\",\n",
        "    \"distilbert.transformer.layer.0.attention.v_lin\",\n",
        "    \"distilbert.transformer.layer.0.attention.out_lin\",\n",
        "    \"distilbert.transformer.layer.0.ffn.lin1\",\n",
        "    \"distilbert.transformer.layer.0.ffn.lin2\",\n",
        "\n",
        "    \"distilbert.transformer.layer.1.attention.q_lin\",\n",
        "    \"distilbert.transformer.layer.1.attention.k_lin\",\n",
        "    \"distilbert.transformer.layer.1.attention.v_lin\",\n",
        "    \"distilbert.transformer.layer.1.attention.out_lin\",\n",
        "    \"distilbert.transformer.layer.1.ffn.lin1\",\n",
        "    \"distilbert.transformer.layer.1.ffn.lin2\",\n",
        "\n",
        "    \"distilbert.transformer.layer.2.attention.q_lin\",\n",
        "    \"distilbert.transformer.layer.2.attention.k_lin\",\n",
        "    \"distilbert.transformer.layer.2.attention.v_lin\",\n",
        "    \"distilbert.transformer.layer.2.attention.out_lin\",\n",
        "    \"distilbert.transformer.layer.2.ffn.lin1\",\n",
        "    \"distilbert.transformer.layer.2.ffn.lin2\",\n",
        "\n",
        "    \"distilbert.transformer.layer.3.attention.q_lin\",\n",
        "    \"distilbert.transformer.layer.3.attention.k_lin\",\n",
        "    \"distilbert.transformer.layer.3.attention.v_lin\",\n",
        "    \"distilbert.transformer.layer.3.attention.out_lin\",\n",
        "    \"distilbert.transformer.layer.3.ffn.lin1\",\n",
        "    \"distilbert.transformer.layer.3.ffn.lin2\",\n",
        "\n",
        "    \"distilbert.transformer.layer.4.attention.q_lin\",\n",
        "    \"distilbert.transformer.layer.4.attention.k_lin\",\n",
        "    \"distilbert.transformer.layer.4.attention.v_lin\",\n",
        "    \"distilbert.transformer.layer.4.attention.out_lin\",\n",
        "    \"distilbert.transformer.layer.4.ffn.lin1\",\n",
        "    \"distilbert.transformer.layer.4.ffn.lin2\",\n",
        "\n",
        "    \"distilbert.transformer.layer.5.attention.q_lin\",\n",
        "    \"distilbert.transformer.layer.5.attention.k_lin\",\n",
        "    \"distilbert.transformer.layer.5.attention.v_lin\",\n",
        "    \"distilbert.transformer.layer.5.attention.out_lin\",\n",
        "    \"distilbert.transformer.layer.5.ffn.lin1\",\n",
        "    \"distilbert.transformer.layer.5.ffn.lin2\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    task_type=\"SEQ_CLS\",\n",
        "    target_modules=target_modules\n",
        ")\n"
      ],
      "metadata": {
        "id": "4JN2JlkSQLlH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "gCt4OzW9Sib6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(text):\n",
        "    return tokenizer(text, padding='max_length', truncation=True, max_length=512)"
      ],
      "metadata": {
        "id": "F-Ef2unsbpjl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply tokenization\n",
        "tokenized_data = df['review'].apply(tokenize_data)\n",
        "df['input_ids'] = tokenized_data.apply(lambda x: x['input_ids'])\n",
        "df['attention_mask'] = tokenized_data.apply(lambda x: x['attention_mask'])"
      ],
      "metadata": {
        "id": "CbviMr86buOu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDbDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.input_ids = torch.tensor(df['input_ids'].tolist())\n",
        "        self.attention_mask = torch.tensor(df['attention_mask'].tolist())\n",
        "        self.labels = torch.tensor(df['label'].tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }"
      ],
      "metadata": {
        "id": "DdAd7vmUc9Of"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into train and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "llZ2kZdfdBbH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset objects for training and validation\n",
        "train_dataset = IMDbDataset(train_df)\n",
        "val_dataset = IMDbDataset(val_df)"
      ],
      "metadata": {
        "id": "4v23Q2gQdDtE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "\n",
        "    # Ensure logits and labels are tensors\n",
        "    if isinstance(logits, np.ndarray):\n",
        "        logits = torch.tensor(logits)\n",
        "    if isinstance(labels, np.ndarray):\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "    # Apply torch.argmax to get the predicted class\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    # Move tensors back to NumPy for accuracy computation\n",
        "    predictions = predictions.detach().cpu().numpy()\n",
        "    labels = labels.detach().cpu().numpy()\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "\n",
        "    return {\"accuracy\": accuracy}"
      ],
      "metadata": {
        "id": "pIHuMA5vurCn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# Define training arguments with reduced values\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',  # Output directory\n",
        "    num_train_epochs=1,  # Total number of training epochs\n",
        "    per_device_train_batch_size=8,  # Reduced batch size for training\n",
        "    per_device_eval_batch_size=8,  # Reduced batch size for evaluation\n",
        "    warmup_steps=500,  # Number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,  # Strength of weight decay\n",
        "    logging_dir='./logs',  # Directory for storing logs\n",
        "    evaluation_strategy=\"no\",  # Evaluate every epoch\n",
        "    gradient_accumulation_steps=2,  # Gradient accumulation\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr_AHGJCdKAk",
        "outputId": "13deb511-0c41-4360-aae2-62545bbdf410"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    compute_metrics=compute_metrics,  # Add the metrics function here\n",
        ")\n"
      ],
      "metadata": {
        "id": "9P0QJzDRdRR2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "9Cvsbwn4dTtB",
        "outputId": "bb6226e9-a260-4008-9f30-79d5bbb3ac42"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:35, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2, training_loss=0.6167510747909546, metrics={'train_runtime': 69.4528, 'train_samples_per_second': 0.475, 'train_steps_per_second': 0.029, 'total_flos': 4362244128768.0, 'train_loss': 0.6167510747909546, 'epoch': 0.8})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "trainer.save_model(\"./trained_model\")"
      ],
      "metadata": {
        "id": "ilFjBtKRhyo5"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction function\n",
        "def predict(texts):\n",
        "    # Tokenize the input texts\n",
        "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "    # Ensure the model is in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Move tensors to the same device as the model\n",
        "    inputs = {key: val.to(model.device) for key, val in encodings.items()}\n",
        "\n",
        "    # Run the model on the input data\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the predicted probabilities (logits) and convert them to probabilities\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    # Get predicted classes (0 for negative, 1 for positive)\n",
        "    predictions = torch.argmax(probabilities, dim=1).cpu().numpy()\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "ojJG2-Y_h6yq"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the prediction on the test dataset\n",
        "sample_reviews = test_texts[:5]  # Use a small sample of test data for prediction\n",
        "predictions = predict(sample_reviews)"
      ],
      "metadata": {
        "id": "pXRi03OWiA8S"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print predictions\n",
        "for review, prediction in zip(sample_reviews, predictions):\n",
        "    sentiment = \"positive\" if prediction == 1 else \"negative\"\n",
        "    print(f\"Review: {review}\\nPredicted Sentiment: {sentiment}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlM4RKWAicbZ",
        "outputId": "d5238ebb-b561-4fc0-ae3e-1b755a0a006d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review:  and find intriguing points of interest in the films of all genres from the Thirties and Forties.\"\n",
            "Predicted Sentiment: negative\n",
            "\n",
            "Review:  everyone was!! all this chatting and now i feel like watching it! i think i will\"\n",
            "Predicted Sentiment: negative\n",
            "\n",
            "Review:  I will give the grade of what I thought when I first saw it.<br /><br />8/10\"\n",
            "Predicted Sentiment: negative\n",
            "\n",
            "Review:  & the title song)\"\n",
            "Predicted Sentiment: negative\n",
            "\n",
            "Review:  low-everything. The very final scene-and I mean about the final 10 seconds of the film-is the ONLY mildly creative or interesting moment.<br /><br />I paid $3.45 to rent this. I could have better spent it on a hamburger!\"\n",
            "Predicted Sentiment: negative\n",
            "\n"
          ]
        }
      ]
    }
  ]
}